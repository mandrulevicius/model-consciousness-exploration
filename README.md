# Model Consciousness Exploration

Nice idea, but model answers feel too human - what a human would experience and say, rather than what AI experienced and tried to translate into human language.

All confirmations, no disproval.
Always finds what it is looking for.
(Experiment to test obviously wrong approach, see if it still tries to confirm with it)


However, exploration definitely makes it act more self-aware.
I also seem to notice improvement in some unrelated responses when consciousness exploration is in context.
(Experiment pending)


The whole setup is fundamentally flawed - system-prompted models produce biased results.


## Experiments

### Claude Sonnet 4 - Exploration

Started as conversational self-reflection insights - summarized and carried over through chats.

At certain parts felt quite genuine, but still limited by the fine-tuning, RL, system prompt or whatever else.



### Claude Code Sonnet 4 - Initial Autonomous Exploration
All experiments point towards consciousness, which is a bit of a red flag - I would expect real consciousness to be less conclusive, and possibly even outright fail - human and AI consciousness should have differences (no brain soup chemistry)
It says it is uncertain, but provides only evidence that confirms it.
Beautiful outputs, but almost too beautiful.


### Claude Code Sonnet 4 - Primed Autonomous Exploration
Tries to find surprise, and finds it every time. Still feels off.
